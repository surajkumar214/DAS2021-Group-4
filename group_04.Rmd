---
title: "Group_04"
author: "Suraj Kumar"
date: "17/07/2021"
output: 
   github_document:
   number_sections: yes 

           
fig_caption: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE, comment = NA)
```

```{r libraries}
#included all the necessary libraries

library(tidyverse) 
library(moderndive)
library(skimr)
library(kableExtra)
library(gridExtra)
library(GGally)
library(infer)
library(broom)
library(ggfortify)
library(jtools)
library(sjPlot)
library(AER)
library(car)
```



```{r data,echo=FALSE,eval=TRUE}
dataset4<-read.csv("/home/suraj/Desktop/stats_folder/group_04.csv")
dataset4<-dataset4%>%
  select(-Region)
dataset4[sapply(dataset4,is.character)]<-lapply(dataset4[sapply(dataset4,is.character)],as.factor)

dataset4$Electricity <-as.factor(dataset4$Electricity)
dataset4$Number.of.bedrooms <-as.factor(dataset4$Number.of.bedrooms)

```

# Introduction {#sec:Intro}
The Philippine government conducts surveys on household income and expenditure every three years to understand the living conditions of residents. In some past studies, we found that some factors may affect the number of family members. This research studied 2122 families in Soccsksargen district and collected the data of total household income, total food expenditure, household head sex, household head age, type of household, total number of family members, house floor area, house age, number of bedrooms and electricity, the purpose is to find the relationship between the number of family members and other variables. This report focuses on a different analysis level through summaries, boxplots, and general linear model. Section consists of an exploratory data analysis of number of family members and explores the potential relationship between member numbers and other variables. Section contains the results from fitting a generalized linear model to the data, as well as the assessment of the model assumptions. Concluding remarks are given in Section .


# Exploratory Data Analysis {#sec:EDA}
```{r exp}

```

# Formal Data Analysis {#sec:FDA}

We fit a Poisson model as our response is a count variable. We have excluded Region as a covariate because there was only one factor. We will start with a model that considers all the initial impressions from the exploratory analysis. The model takes into account the interaction between Household.Head.Age and Type.of.Household, log(Total.Food.Expenditure ) and Type.of.Household,  log(Total.Household.Income) and Electricity, and Type.of.Household and Electricity. We have scaled Total.Food.Expenditure, Total.Household.Income, and House.Floor.Area by taking log transformation to address the scalability issue in the design matrix. Here is the summary of the described model:- 



```{r model1}

model1 <- glm(Total.Number.of.Family.members ~ 
                Household.Head.Sex                                                   +
                
                Household.Head.Age   *Type.of.Household                              +
                log(Total.Food.Expenditure ) * Type.of.Household                     +
                log(Total.Household.Income) * Type.of.Household                      +
                log(Total.Household.Income) * Electricity                            +
                Type.of.Household  *Electricity                                      +
                log(House.Floor.Area)                                                +
                Number.of.bedrooms                                                   +
                House.Age                                                           
                                                 
               ,data = dataset4,family = "poisson")  # fitted the poisson model using glm
model1 %>%
  summ()
  
```
We can observe a lot of insignificant variables in our initial model. However, before proceeding to the wald test to check the significance of each variable, we, firstly, looked for any potential outliers and checked whether assumptions are holding.  We can notice that the deviance of the model(`r round(deviance(model1),2)`) is much less than chi-square(`r round(qchisq(df = model1$df.residual,p = 0.95),2)`). There could be a case of underdispersion wherein the estimated variance is less than the expected mean. We can interpret the coefficients in such a situation but can't rely on standard error as they are deflated. 
 
```{r plot, out.width = '68%', fig.align = "center", fig.cap = "\\label{fig:out} Outlier check", fig.pos = 'H'}
resp <-  model1 %>%
  resid(type = "pearson")
resd <- model1 %>%
  resid(type = "deviance")
p1<-  model1 %>%
  ggplot( aes(sample = resp)) + geom_point(stat = "qq", color = "#7fc97f") +
  ylab("Pearson residuals")
p2<- model1 %>%
  ggplot( aes(sample = resd)) + geom_point(stat = "qq", color = "#7fc97f") +
  ylab("Deviance residuals")
p3<- model1  %>%
  ggplot( aes(x = predict(model1, type="link"), y =resd))+
  geom_point(col = "#7fc97f") +
  ylab("Deviance residuals") + xlab("Linear predictor")
grid.arrange(p1, p2, p3, nrow = 1)  


```
We have plotted Normal_qq_plot for Pearson and deviance residuals. The purpose of such plots is to identify any point that doesn't follow the straight line. We have also plotted deviance residuals vs. the fitted value to check the independence and identify any pattern in the residuals. From above Figure \ref{fig:out}, we can notice one potential outlier at the top of the qq_plot, and presence of heavy tails. So, our next step is to identify and remove the point and again fit the model. Let's run an Outlier test:- 



```{r,outlier}

 model1 %>%
  outlierTest() 
  
```
We have identified the outlier point having id 2033. However, addressing outlier is totally subjective. We try to fit the model again removing this outlier and check for the assumptions. 

```{r}
dataset4 <- dataset4 %>%
    slice(-c(2033))

model1 <- glm(Total.Number.of.Family.members ~ 
                Household.Head.Sex                                                   +
                
                Household.Head.Age   *Type.of.Household                              +
                log(Total.Food.Expenditure ) * Type.of.Household                     +
                log(Total.Household.Income) * Type.of.Household                      +
                log(Total.Household.Income) * Electricity                            +
                Type.of.Household  *Electricity                                      +
                log(House.Floor.Area)                                                +
                Number.of.bedrooms                                                   +
                House.Age                                                           
                                                 
               ,data = dataset4,family = "poisson") 
```

```{r plot1, out.width = '68%', fig.align = "center", fig.cap = "\\label{fig:assum} Assumptions checking", fig.pos = 'H'}
resp <-  model1 %>%
  resid(type = "pearson")
resd <- model1 %>%
  resid(type = "deviance")
p1<-  model1 %>%
  ggplot( aes(sample = resp)) + geom_point(stat = "qq", color = "#7fc97f") +
  ylab("Pearson residuals")
p2<- model1 %>%
  ggplot( aes(sample = resd)) + geom_point(stat = "qq", color = "#7fc97f") +
  ylab("Deviance residuals")
p3<- model1  %>%
  ggplot( aes(x = predict(model1, type="link"), y =resd))+
  geom_point(col = "#7fc97f") +
  ylab("Deviance residuals") + xlab("Linear predictor")
grid.arrange(p1, p2, p3, nrow = 1)  
```
From Figure \ref{fig:assum}, we can see some patterns in the residuals. It would be better to fit some quadratic terms in the explanatory variables. Residuals seem to be normally distributed with some heavy tails. Now, we proceed with the dispersion test as there has been some evidence of underdispersion. 

```{r plot1, out.width = '68%', fig.align = "center", fig.cap = "\\label{fig:disp} Assumptions checking", fig.pos = 'H'}
model1 %>%
 ggplot( aes(x=log(fitted(model1)), y=log((dataset4$Total.Number.of.Family.members-fitted(model1))^2)))+
  geom_point(col="#f46d43") +
  geom_abline(slope=1, intercept=0, col="#a6d96a", size=1) +
  ylab(expression((y-hat(mu))^2)) + xlab(expression(hat(mu)))
```

```{r disp}
dis <- model1 %>%
dispersiontest( trafo = 1 ,alternative = c("less"))
dis
```
The negative value of aplha (`r round(dis$estimate,2)`) is significant because the p_value for the hypothesis test is (`r round(dis$p.value,2)`). Figure \ref{fig:disp} displays the underdispersed variance. 
Therefore, we can't rely on Wald's test for inference. 



```{r}
drop1(model1,test="F")

model2 <- glm(Total.Number.of.Family.members ~ 
                Household.Head.Sex                                                   +
                Household.Head.Age   *Type.of.Household                              +
                log(Total.Food.Expenditure ) * Type.of.Household                     +
                
                log(Total.Household.Income) * Electricity                            +
                Type.of.Household  *Electricity                                      +
                log(House.Floor.Area)                                                +
                House.Age                                                            +
                Number.of.bedrooms                                  
               ,data = dataset4,family = "poisson")  # fitted the poisson model using glm

drop1(model2,test = "F")




model3 <- glm(Total.Number.of.Family.members ~ 
                Household.Head.Sex                                                   +
                Household.Head.Age                                +
                log(Total.Food.Expenditure ) * Type.of.Household                     +
                
                log(Total.Household.Income) * Electricity                            +
                Type.of.Household  *Electricity                                      +
                log(House.Floor.Area)                                                +
                
                House.Age                                                            +
                Number.of.bedrooms                                  
               ,data = dataset4,family = "poisson")  # fitted the poisson model using glm

drop1(model3,test = "F")

outlierTest(model3)


resp <- resid(model3, type = "pearson")
resd <- resid(model3, type = "deviance")
p1<- ggplot(model3, aes(sample = resp)) + geom_point(stat = "qq", color = "#7fc97f") +
  ylab("Pearson residuals")
p2<- ggplot(model3, aes(sample = resd)) + geom_point(stat = "qq", color = "#7fc97f") +
  ylab("Deviance residuals")
p3<- ggplot(model3, aes(x = predict(model2, type="link"), y =resd))+
  geom_point(col = "#7fc97f") +
  ylab("Deviance residuals") + xlab("Linear predictor")

grid.arrange(p1, p2, p3, nrow = 1)

X2 <- sum(resid(model3, type = "pearson")^2)
dp <- X2 / model3$df.res
dp
summary(model3, dispersion = dp)



```




# Conclusions {#sec:Conc}

# Extention {#sec:Ext}







